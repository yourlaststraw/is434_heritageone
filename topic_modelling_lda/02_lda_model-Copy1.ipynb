{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de007d66-35e1-402e-a294-79db09d552b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas nltk scikit-learn gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c56406-293c-4cf7-a30f-e9744eaea495",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b0dda-310e-4588-95d4-6a2cd72cd06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download NLTK stop words (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load necessary NLTK resources\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "949a8af2-923f-4c68-9384-e19008b5bfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.47365321060762494\n",
      "Topic 0: 0.041*\"india\" + 0.034*\"taste\" + 0.019*\"little\" + 0.014*\"price\" + 0.012*\"staff\" + 0.010*\"visit\" + 0.010*\"authentic\"\n",
      "Topic 1: 0.033*\"mutton\" + 0.028*\"bryan\" + 0.027*\"chicken\" + 0.020*\"taste\" + 0.018*\"mangalore\" + 0.017*\"order\" + 0.014*\"like\"\n",
      "Topic 2: 0.030*\"india\" + 0.020*\"taste\" + 0.016*\"great\" + 0.014*\"delicious\" + 0.011*\"south\" + 0.011*\"ve\" + 0.011*\"authentic\"\n",
      "Topic 3: 0.310*\"nan\" + 0.013*\"great\" + 0.013*\"chicken\" + 0.011*\"fish\" + 0.009*\"taste\" + 0.008*\"carry\" + 0.008*\"absence\"\n",
      "Topic 4: 0.025*\"mangalore\" + 0.014*\"order\" + 0.014*\"britain\" + 0.012*\"visit\" + 0.011*\"authentic\" + 0.009*\"mutton\" + 0.009*\"quinine\"\n",
      "\n",
      "\n",
      "\n",
      "Topic 0: india, taste, little, price, staff, visit, authentic\n",
      "Topic 1: mutton, bryan, chicken, taste, mangalore, order, like\n",
      "Topic 2: india, taste, great, delicious, south, ve, authentic\n",
      "Topic 3: nan, great, chicken, fish, taste, carry, absence\n",
      "Topic 4: mangalore, order, britain, visit, authentic, mutton, quinine\n",
      "<gensim.interfaces.TransformedCorpus object at 0x32e6ec290>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.models import Phrases, CoherenceModel\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define dictionary for standardizing specific words\n",
    "standardize_dict = {\n",
    "    'indian': 'india',\n",
    "    'indain': 'india',\n",
    "    'indians': 'india',\n",
    "    'ambiance': 'ambience',\n",
    "    'knew': 'know',\n",
    "    'lets': 'let',\n",
    "    'looked': 'look',\n",
    "    'mangaloreaan': 'mangalore',\n",
    "    'mangalorean': 'mangalore',\n",
    "    'mangaloren': 'mangalore',\n",
    "    'mangaloreso': 'mangalore',\n",
    "    'manglorean': 'mangalore',\n",
    "    'manglorian': 'mangalore',\n",
    "    'gd': 'good'\n",
    "    # Add other words as needed\n",
    "}\n",
    "\n",
    "# Load data\n",
    "file_path = './00_google_maps_reviews_all_pages.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Custom stop words\n",
    "custom_stop_words = {'good', 'nice', 'place', 'food', 'restaurant', 'service', 'try'}\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Preprocessing function with additional steps\n",
    "def preprocess_text_spacy(text):\n",
    "    # Remove non-alphabetic characters and lowercase the text\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I | re.A).lower()\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initial tokenization and stop word removal\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop and token.lemma_.strip() != '' and token.lemma_ not in custom_stop_words\n",
    "    ]\n",
    "    \n",
    "    # Step 1: Correct spelling for each word\n",
    "    corrected_words = [str(TextBlob(word).correct()) for word in tokens]\n",
    "    \n",
    "    # Step 2: Lemmatize the corrected words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in corrected_words]\n",
    "    \n",
    "    # Step 3: Standardize words based on the dictionary\n",
    "    standardized_words = [standardize_dict.get(word, word) for word in lemmatized_words]\n",
    "    \n",
    "    return standardized_words\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_text'] = df['description'].apply(lambda x: preprocess_text_spacy(str(x)))\n",
    "\n",
    "# Tokenize reviews for bigram and trigram models\n",
    "tokenized_reviews = df['cleaned_text'].tolist()\n",
    "\n",
    "# Create bigram and trigram models\n",
    "bigram = Phrases(tokenized_reviews, min_count=2, threshold=80)\n",
    "trigram = Phrases(bigram[tokenized_reviews], threshold=80)\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)\n",
    "\n",
    "# Apply bigrams and trigrams\n",
    "tokenized_reviews = [bigram_mod[review] for review in tokenized_reviews]\n",
    "tokenized_reviews = [trigram_mod[review] for review in tokenized_reviews]\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(tokenized_reviews)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_reviews]\n",
    "\n",
    "# LDA model with tuned parameters\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    num_topics=5,         # Experiment with number of topics\n",
    "    id2word=dictionary,\n",
    "    passes=90,            # Increase passes to allow model to refine further\n",
    "    alpha='auto',         # Auto-adjust alpha and eta for topic sparsity\n",
    "    eta='auto'\n",
    ")\n",
    "\n",
    "# Calculate coherence\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_reviews, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence Score: {coherence_score}')\n",
    "\n",
    "# Display topics\n",
    "topics = lda_model.print_topics(num_words=7)\n",
    "for idx, topic in topics:\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "# Display topics without weights\n",
    "topics = lda_model.print_topics(num_words=7)\n",
    "for idx, topic in topics:\n",
    "    # Extract only the words from the topic string by splitting and filtering out weights\n",
    "    words = [word.split('*')[1].replace('\"', '') for word in topic.split(' + ')]\n",
    "    # Join the words into a single string and print the topic number with words only\n",
    "    print(f\"Topic {idx}: {', '.join(words)}\")\n",
    "    \n",
    "# Document-topic distribution\n",
    "doc_topics = lda_model.get_document_topics(corpus)\n",
    "print(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38867bb-10e6-49e6-8f58-137e4a7855af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display per-document topic distributions\n",
    "for doc_num, topics in enumerate(lda_model.get_document_topics(corpus)):\n",
    "    print(f\"Document {doc_num}:\")\n",
    "    for topic, prob in topics:\n",
    "        print(f\"  Topic {topic} - Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e930b6-6186-424b-8fae-8a3d11d10afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "# Prepare the visualization data for pyLDAvis\n",
    "vis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "# Display the visualization in a notebook or as an HTML file\n",
    "pyLDAvis.display(vis_data)\n",
    "# Alternatively, save the visualization as an HTML file\n",
    "pyLDAvis.save_html(vis_data, 'lda_visualization.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
